# Проект: CPC Auto Helper (Стартовый этап MVP)

## 1. Общее Описание

**Цель:** Разработка веб-приложения (SaaS) для помощи специалистам по контекстной рекламе Яндекс.Директ, автоматизирующего сбор и предоставляющего инструменты для анализа еженедельной статистики кампаний. MVP фокусируется на сборе данных, их просмотре и выгрузке в CSV.

**Технологический стек:**
*   **Backend:** Python 3.x, Flask
*   **Структура:** Flask Blueprints
*   **Работа с БД:** Flask-SQLAlchemy (ORM), SQLAlchemy
*   **База данных:** SQLite (файл `tokens.db`)
*   **HTTP Запросы:** Requests
*   **Парсинг/Генерация CSV:** Модуль `csv` Python, `io`
*   **Конфигурация:** python-dotenv
*   **Frontend:** HTML (через Jinja2), базовый CSS.

**Окружение Разработки:**
*   **ОС:** Windows 10/11
*   **Python:** Версия 3.x (установлен)
*   **Виртуальное окружение:** Используется `venv`
*   **Менеджер пакетов:** `pip`

**Структура проекта:** (Без изменений по сравнению с предыдущей версией README)
```
C:\python\cpc_auto_star\
├── venv\
├── instance/
│   └── tokens.db
├── app/
│   ├── __init__.py
│   ├── models.py
│   ├── auth/
│   ├── reports/
│   └── templates/ # Папка для Jinja2 шаблонов - ИСПОЛЬЗУЕТСЯ
├── utils/
│   └── create_test_campaigns.py
├── static/ # Папка для статических файлов - **НОВОЕ**
│   └── css/
│       └── style.css # Базовый CSS - **НОВОЕ**
├── .env
├── .gitignore
├── requirements.txt
├── run.py
├── BUSINESS_LOGIC.md # Документ с бизнес-логикой - СОЗДАН/ОБНОВЛЕН
└── PROJECT_README.md # Этот файл
```

## 2. Текущий Статус (Что уже сделано)

*   [x] Настроена среда разработки.
*   [x] Реализован полный цикл Яндекс OAuth с хранением токенов в БД и автообновлением.
*   [x] Код реорганизован с использованием Flask Blueprints (`auth`, `reports`).
*   [x] Реализован вывод списка кампаний пользователя.
*   [x] Реализован механизм получения отчетов из API Reports v5 (включая ожидание и парсинг).
*   [x] Создана утилита для генерации тестовых кампаний.
*   [x] Создан документ `BUSINESS_LOGIC.md`, описывающий функционал.
*   [ ] Спроектированы таблицы БД для хранения **еженедельной** статистики.
*   [ ] Реализованы функции ручного запуска сбора/обновления данных за **4 недели**.
*   [ ] Реализован интерфейс (Jinja2+CSS) для просмотра собранных **еженедельных** данных.
*   [ ] Реализована функция **скачивания CSV** по выбранным срезам из локальной БД.
*   [ ] Настроен базовый CSS и шаблонизатор Jinja2.

## 3. ЗАДАЧА ДЛЯ CURSOR - Веха 4: MVP - Еженедельный Сбор и Анализ Данных

**Цель:** Реализовать основной функционал MVP, позволяющий пользователю собирать (вручную), хранить (понедельно) и анализировать (просмотр + выгрузка CSV) статистику своих рекламных кампаний по ключевым срезам.

**Разбивка на подзадачи:**

**Подзадача 4.1: Проектирование и Создание БД для Еженедельной Статистики**

*   **Действия:**
    1.  В `app/models.py` определить SQLAlchemy-модели для хранения еженедельных данных. Понадобятся отдельные таблицы (модели) для каждого среза, например:
        *   `WeeklyCampaignStat(db.Model)`: `week_start_date`, `campaign_id`, `impressions`, `clicks`, `cost`... (агрегированные данные по кампании)
        *   `WeeklyPlacementStat(db.Model)`: `week_start_date`, `campaign_id`, `placement` (название площадки), `ad_network_type` (тип сети), `impressions`, `clicks`, `cost`...
        *   `WeeklySearchQueryStat(db.Model)`: `week_start_date`, `campaign_id`, `ad_group_id`, `query`, `impressions`, `clicks`, `cost`...
        *   `WeeklyGeoStat(db.Model)`: `week_start_date`, `campaign_id`, `location_id`, `impressions`, `clicks`, `cost`...
        *   `WeeklyDeviceStat(db.Model)`: `week_start_date`, `campaign_id`, `device_type`, `impressions`, `clicks`, `cost`...
        *   `WeeklyDemographicStat(db.Model)`: `week_start_date`, `campaign_id`, `gender`, `age_group`, `impressions`, `clicks`, `cost`...
    2.  Определить первичные ключи (вероятно, составные) и базовые индексы (по `week_start_date`, `campaign_id`). `week_start_date` должно хранить дату понедельника соответствующей недели.
    3.  Использовать `with app.app_context(): db.create_all()` (временно в `run.py` или отдельном скрипте) для создания таблиц в `tokens.db` (или лучше в отдельном файле `stats.db`?). *Решение: Пока оставим в `tokens.db` для простоты MVP.*
*   **Проверка:** Таблицы успешно созданы в БД.

**Подзадача 4.2: Адаптация Функций Сбора Данных API Reports**

*   **Действия:**
    1.  Модифицировать существующие функции в `app/reports/utils.py` (или создать новые), чтобы они запрашивали данные из API Reports за **недельный период** (Пн-Вс), а не за произвольный/последние N дней. Понадобится функция для расчета дат начала и конца недели.
    2.  Убедиться, что функции запрашивают все необходимые `FieldNames` для каждого нужного среза (Кампания, Площадки, Запросы, Гео, Устройства, Демография). Возможно, потребуется несколько разных запросов к API Reports для получения всех срезов по одной кампании за неделю.
    3.  Функции должны возвращать распарсенные данные в удобном формате (например, список словарей).
*   **Проверка:** Функции корректно запрашивают и возвращают данные за указанную неделю.

**Подзадача 4.3: Реализация Ручного Запуска Сбора/Обновления**

*   **Действия:**
    1.  В `app/reports/utils.py` создать **основную функцию сбора данных**, например, `collect_weekly_stats_for_last_n_weeks(n_weeks=4)`:
        *   Получает `access_token`, `client_login`.
        *   Определяет даты начала для последних `n_weeks` полных недель.
        *   Получает список активных кампаний пользователя.
        *   **В цикле по неделям и кампаниям:**
            *   Вызывает адаптированные функции из п.4.2 для получения всех необходимых срезов.
            *   **Обрабатывает полученные данные:** Перед записью в БД удаляет старые данные за эту неделю и кампанию (для обеспечения перезаписи при обновлении).
            *   Записывает новые данные в соответствующие таблицы БД (`Weekly*Stats`).
            *   Добавить логирование и обработку ошибок API/парсинга. Учесть лимиты API (добавить `time.sleep`).
    2.  В `app/reports/routes.py` создать два **POST-роута**, доступные авторизованному пользователю:
        *   `/reports/load_initial_data` (эндпоинт `reports.load_initial`): Вызывает `collect_weekly_stats_for_last_n_weeks(n_weeks=4)`.
        *   `/reports/update_data` (эндпоинт `reports.update_data`): Также вызывает `collect_weekly_stats_for_last_n_weeks(n_weeks=4)`.
    3.  В интерфейсе (например, на главной странице отчетов) добавить **кнопки**, отправляющие POST-запросы на эти роуты.
    4.  Использовать `flash` для сообщений о начале, завершении или ошибке процесса сбора данных. Сделать редирект обратно на страницу отчетов.
*   **Проверка:** Нажатие кнопок запускает процесс сбора, данные появляются/обновляются в БД, пользователь видит сообщения.

**Подзадача 4.4: Настройка Базового UI (Jinja2 + CSS)**

*   **Действия:**
    1.  Создать базовый шаблон `app/templates/base.html` с основной разметкой (шапка, область контента, подключение CSS).
    2.  Создать папку `app/static/css` и файл `style.css`. Добавить минимальные стили для таблиц (`border`, `padding`, `width: 100%`, `text-align: left`), кнопок, сообщений `flash`.
    3.  Перевести все существующие роуты на использование `render_template('имя_шаблона.html', **контекст)`, передавая необходимые данные в шаблон. Наследовать шаблоны страниц от `base.html`.
    4.  Реализовать отображение `flash`-сообщений в `base.html`.
*   **Проверка:** Приложение имеет единообразный (хотя и простой) внешний вид, таблицы читаемы.

**Подзадача 4.5: Интерфейс Просмотра Еженедельных Данных**

*   **Действия:**
    1.  Создать шаблон `app/templates/reports/campaign_detail.html`.
    2.  Модифицировать роут `/reports/campaigns`, чтобы он передавал список кампаний в шаблон `campaign_list.html`. Ссылки с кампаний ведут на новый роут детализации.
    3.  Создать роут `/reports/campaign/<int:campaign_id>/view` (эндпоинт `reports.view_campaign_detail`).
    4.  В этом роуте:
        *   Получить `campaign_id`.
        *   Определить даты начала последних 4 недель.
        *   Сделать запросы к **локальной БД** для извлечения сохраненной статистики (`Weekly*Stats`) по этой кампании за эти 4 недели.
        *   Агрегировать данные за 4 недели для отображения общих показателей.
        *   Передать данные в шаблон `campaign_detail.html`.
    5.  В шаблоне `campaign_detail.html`:
        *   Отобразить общие показатели кампании за 4 недели.
        *   Отобразить таблицы с данными по основным срезам (Площадки, Запросы, Гео, Устройства, Демография), агрегированными за 4 недели.
        *   (Опционально) Добавить переключатель для просмотра данных по конкретной неделе.
        *   Добавить форму для выбора срезов для скачивания CSV (см. 4.6).
*   **Проверка:** Пользователь может выбрать кампанию и увидеть агрегированные данные и данные по срезам за последние 4 недели, загруженные из локальной БД.

**Подзадача 4.6: Реализация Скачивания CSV**

*   **Действия:**
    1.  В шаблоне `campaign_detail.html` добавить форму (метод GET или POST), содержащую:
        *   Чекбоксы для выбора срезов (`name="selected_slices"`, `value="placements"`, `value="queries"` и т.д.).
        *   Возможно, выбор периода (пока только "last_4_weeks").
        *   Кнопку "Скачать CSV".
    2.  Создать роут `/reports/campaign/<int:campaign_id>/download_csv` (эндпоинт `reports.download_csv`), принимающий параметры из формы (ID кампании, срезы).
    3.  В роуте:
        *   Получить параметры.
        *   Для каждого выбранного среза сделать запрос к **локальной БД** (`Weekly*Stats`) для получения данных по кампании за последние 4 недели.
        *   Сформировать CSV-строки в памяти (`io.StringIO`, `csv.writer`). Можно сделать один файл с несколькими разделами или генерировать zip-архив с несколькими CSV (сложнее, для MVP достаточно одного файла или первого выбранного среза). *Решение MVP: генерировать CSV для **первого** выбранного среза.*
        *   Создать объект `Response` Flask, указав данные CSV, MIME-тип (`text/csv`) и заголовок `Content-Disposition` для скачивания файла с понятным именем.
        *   Вернуть этот `Response`.
*   **Проверка:** Пользователь выбирает срез, нажимает кнопку, скачивается CSV-файл с корректными данными из локальной БД.

**Важные Замечания для Cursor:**
*   Фокус на **еженедельной** гранулярности данных при запросах к API и хранении в БД.
*   Реализация **ручного** запуска сбора/обновления данных через UI.
*   Генерация CSV **на лету** из данных **локальной БД**.
*   Использование **Jinja2** и базового **CSS**.
*   Тщательное **логирование** фоновых операций и **обработка ошибок** API.
*   Учет **ограничений API** (таймауты, лимиты запросов).

## 4. Следующие Шаги (После Вехи 4 - MVP)**

*   [ ] **Веха 5: Автоматизация и Действия:** Автоматический еженедельный сбор, Реализация блокировки площадок/минусации.
*   [ ] **Веха 6: Улучшения и Доп. Функции:** Динамический анализ, произвольные периоды, отчет по объявлениям, контроль бюджета, улучшения UX/UI, переход на фоновые задачи, шифрование.
*   [ ] **Веха 7: Интеграция AI.**

